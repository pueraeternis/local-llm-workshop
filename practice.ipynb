{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f242ced7",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç—ã –∏ –£—Ç–∏–ª–∏—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b520d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤\n",
    "results = []\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ VRAM –∏ Garbage Collector\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "    print(\"üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"–í—ã–≤–æ–¥ –∑–∞–Ω—è—Ç–æ–π –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        print(f\"üìä VRAM: –ó–∞–Ω—è—Ç–æ {allocated:.2f} –ì–ë | –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ {reserved:.2f} –ì–ë\")\n",
    "    else:\n",
    "        print(\"‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adc481",
   "metadata": {},
   "source": [
    "## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ (–¢–µ–æ—Ä–∏—è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏\n",
    "def estimate_vram(params_b, quant_type=\"fp16\", context=8192):\n",
    "    bytes_per_param = {\n",
    "        \"fp16\": 2,      # 16 bit = 2 bytes\n",
    "        \"int8\": 1,      # 8 bit = 1 byte\n",
    "        \"int4\": 0.5     # 4 bit = 0.5 bytes (packed)\n",
    "    }\n",
    "    \n",
    "    bpp = bytes_per_param.get(quant_type, 2)\n",
    "    \n",
    "    # –í–µ—Å –º–æ–¥–µ–ª–∏\n",
    "    model_size = params_b * bpp\n",
    "    \n",
    "    # KV-Cache (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: 2 * layers * hidden * context...)\n",
    "    # –ì—Ä—É–±–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: ~0.5 –ú–ë –Ω–∞ 1–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è 7B –º–æ–¥–µ–ª–µ–π –≤ FP16\n",
    "    kv_cache = (context / 1000) * 0.5 * (bpp / 2) \n",
    "    \n",
    "    total = model_size + kv_cache\n",
    "    return f\"–ú–æ–¥–µ–ª—å {params_b}B ({quant_type}): ~{total:.2f} –ì–ë VRAM (–ø—Ä–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {context})\"\n",
    "\n",
    "print(\"--- –í–∞—à–∏ –ª–∏–º–∏—Ç—ã: RTX 4070 (8 GB) ---\")\n",
    "print(estimate_vram(7, \"fp16\")) # –ù–µ –≤–ª–µ–∑–µ—Ç\n",
    "print(estimate_vram(7, \"int4\")) # –í–ª–µ–∑–µ—Ç –ª–µ–≥–∫–æ\n",
    "print(estimate_vram(32, \"int4\")) # –ù–µ –≤–ª–µ–∑–µ—Ç (–Ω—É–∂–µ–Ω A100 –∏–ª–∏ RAM offload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08fd063",
   "metadata": {},
   "source": [
    "## Qwen3-4B –≤ –ø–æ–ª–Ω–æ–º –≤–µ—Å–µ (BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e019115",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "print(f\"üèãÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ {model_id} –≤ –ø–æ–ª–Ω–æ–º –≤–µ—Å–µ (BF16)...\")\n",
    "\n",
    "# –ì—Ä—É–∑–∏–º –≤ bfloat16 (2 –±–∞–π—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä)\n",
    "# device_map=\"auto\" —Å–ø–∞—Å–µ—Ç –æ—Ç –∫—Ä–∞—à–∞, –µ—Å–ª–∏ 8–ì–ë —á—É—Ç—å-—á—É—Ç—å –Ω–µ —Ö–≤–∞—Ç–∏—Ç (–≤—ã–≥—Ä—É–∑–∏—Ç —á–∞—Å—Ç—å –≤ RAM)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print_memory()\n",
    "\n",
    "# –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "input_text = \"Explain the concept of entropy in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "end = time.time()\n",
    "\n",
    "tps = 100 / (end - start)\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å (BF16): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-4B (BF16)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500c8ac",
   "metadata": {},
   "source": [
    "## Qwen3-4B –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è (NF4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "print(f\"ü§è –ó–∞–≥—Ä—É–∑–∫–∞ {model_id} –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ NF4...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print_memory() # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 3 –ì–ë\n",
    "\n",
    "inputs = tokenizer(\"Explain the concept of entropy in simple terms.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "end = time.time()\n",
    "\n",
    "tps = 100 / (end - start)\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å (NF4): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-4B (NF4)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b28e47",
   "metadata": {},
   "source": [
    "## Qwen3-8B (Transformers + Thinking Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1606b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "model_id_8b = \"Qwen/Qwen3-8B\"\n",
    "print(f\"üß† –ó–∞–≥—Ä—É–∑–∫–∞ {model_id_8b} (HF NF4) —Å Thinking Mode...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_8b,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_8b)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Thinking Mode\n",
    "prompt = \"How many 'r's are in the word 'Strawberry'?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º enable_thinking=True\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True \n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "end = time.time()\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏–Ω–≥ (—Ä–∞–∑–¥–µ–ª—è–µ–º –º—ã—Å–ª–∏ –∏ –æ—Ç–≤–µ—Ç)\n",
    "# –í Qwen3 –º—ã—Å–ª–∏ –æ–±—ã—á–Ω–æ –∑–∞–∫–ª—é—á–µ–Ω—ã –≤ —Ç–µ–≥–∏ <think>...</think>\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"\\n--- RAW OUTPUT ---\")\n",
    "print(output_text[:500] + \"...\") # –ü–µ—á–∞—Ç–∞–µ–º –Ω–∞—á–∞–ª–æ, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ç–µ–≥–∏\n",
    "\n",
    "tps = (len(generated_ids[0]) - len(model_inputs.input_ids[0])) / (end - start)\n",
    "print(f\"\\n‚è± –°–∫–æ—Ä–æ—Å—Ç—å (HF 8B): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-8B (HF NF4)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cab865",
   "metadata": {},
   "source": [
    "## Qwen3-8B —á–µ—Ä–µ–∑ Unsloth (–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "# –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Unsloth\n",
    "unsloth_model_id = \"unsloth/Qwen3-8B-bnb-4bit\" \n",
    "\n",
    "print(f\"üêá Unsloth: –ó–∞–≥—Ä—É–∑–∫–∞ {unsloth_model_id}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=unsloth_model_id,\n",
    "    max_seq_length=4096,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # –í–∫–ª—é—á–∞–µ–º –º–∞–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "inputs = tokenizer([\"How many 'r's are in the word 'Strawberry'?\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# –ü—Ä–æ–≥—Ä–µ–≤\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "end = time.time()\n",
    "\n",
    "tps = 512 / (end - start) # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å (Unsloth 8B): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-8B (Unsloth)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f156ef",
   "metadata": {},
   "source": [
    "## Llama.cpp (GGUF & Hybrid Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08849d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Llama-cpp-python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA\n",
    "# –í–ê–ñ–ù–û: –ë–µ–∑ CMAKE_ARGS –æ–Ω–∞ –≤—Å—Ç–∞–Ω–µ—Ç –∫–∞–∫ CPU-only –±–∏–±–ª–∏–æ—Ç–µ–∫–∞.\n",
    "# CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –°–∫–∞—á–∏–≤–∞–µ–º GGUF —Ñ–∞–π–ª (–æ–∫–æ–ª–æ 5 –ì–ë)\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ GGUF (–µ—Å–ª–∏ –µ—â–µ –Ω–µ—Ç)...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"unsloth/Qwen3-8B-GGUF\",\n",
    "    filename=\"qwen3-8b-instruct-q4_k_m.gguf\",\n",
    "    local_dir=\"./models\"\n",
    ")\n",
    "print(f\"–§–∞–π–ª –≥–æ—Ç–æ–≤: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ llama-cpp-python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "clean_memory()\n",
    "\n",
    "print(\"--- –°—Ü–µ–Ω–∞—Ä–∏–π 1: Full GPU (n_gpu_layers=-1) ---\")\n",
    "# -1 –æ–∑–Ω–∞—á–∞–µ—Ç \"–ø–æ–ª–æ–∂–∏—Ç—å –≤—Å–µ —Å–ª–æ–∏ –≤ VRAM\". –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è 7B –Ω–∞ 8GB –∫–∞—Ä—Ç–µ.\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1, \n",
    "    n_ctx=4096,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"–ü–æ—á–µ–º—É Rust —Å—á–∏—Ç–∞—é—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —è–∑—ã–∫–æ–º?\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(\"–û—Ç–≤–µ—Ç (GPU):\", output['choices'][0]['message']['content'])\n",
    "\n",
    "# –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–¥ —Å–∏–º—É–ª—è—Ü–∏–µ–π –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞\n",
    "del llm\n",
    "clean_memory()\n",
    "\n",
    "print(\"\\n--- –°—Ü–µ–Ω–∞—Ä–∏–π 2: Hybrid Mode (CPU + GPU) ---\")\n",
    "# –°–∏–º—É–ª–∏—Ä—É–µ–º —Å–∏—Ç—É–∞—Ü–∏—é, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–µ –≤–ª–µ–∑–∞–µ—Ç (—Å—Ç–∞–≤–∏–º –º–∞–ª–æ —Å–ª–æ–µ–≤ –≤ GPU)\n",
    "llm_hybrid = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=10, # –¢–æ–ª—å–∫–æ 10 —Å–ª–æ–µ–≤ –Ω–∞ –∫–∞—Ä—Ç–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–∞ CPU\n",
    "    n_ctx=4096,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "output = llm_hybrid.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"–ü–æ—á–µ–º—É Python —Ç–∞–∫–æ–π –º–µ–¥–ª–µ–Ω–Ω—ã–π?\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(\"–û—Ç–≤–µ—Ç (Hybrid):\", output['choices'][0]['message']['content'])\n",
    "\n",
    "del llm_hybrid\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d092a6",
   "metadata": {},
   "source": [
    "## Ollama + OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47931e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ Ollama\n",
    "# –ù—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ: docker exec -it rag_ollama ollama pull qwen3:8b\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\" \n",
    ")\n",
    "\n",
    "print(\"üåê –ó–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama (Docker)...\")\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen3:8b\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ DevOps.\"},\n",
    "            {\"role\": \"user\", \"content\": \"–ö–∞–∫ –º–Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å Docker –æ–±—Ä–∞–∑?\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(\"–û—Ç–≤–µ—Ç:\")\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞: {e}\")\n",
    "    print(\"üí° –í—ã–ø–æ–ª–Ω–∏–ª–∏ –ª–∏ –≤—ã 'docker exec -it rag_ollama ollama pull qwen3:8b'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f2787",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ (RTX 4070 Laptop)\")\n",
    "print(tabulate(results, headers=[\"–ú–æ–¥–µ–ª—å\", \"VRAM (GB)\", \"–°–∫–æ—Ä–æ—Å—Ç—å (tok/s)\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3db607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
