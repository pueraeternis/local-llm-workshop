{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506d121b",
   "metadata": {},
   "source": [
    "# –õ–æ–∫–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM. –ü—Ä–∞–∫—Ç–∏–∫–∞\n",
    "\n",
    "## –í–≤–µ–¥–µ–Ω–∏–µ\n",
    "\n",
    "**–¶–µ–ª—å –∑–∞–Ω—è—Ç–∏—è:** \n",
    "–ü–µ—Ä–µ–π—Ç–∏ –æ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º ¬´–∂–µ–ª–µ–∑–µ¬ª. –ú—ã –∏—Å—Å–ª–µ–¥—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è (RTX 4070) –∏ –Ω–∞—É—á–∏–º—Å—è –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å—Ç–µ–∫ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏.\n",
    "\n",
    "### –ß—Ç–æ –º—ã –¥–µ–ª–∞–µ–º –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–∞–∫—Ç–∏–∫–∏:\n",
    "*   **–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ:** –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –º–æ–¥–µ–ª–µ–π —Å–µ–º–µ–π—Å—Ç–≤–∞ **Qwen3 (4B –∏ 8B)** –º—ã —É–≤–∏–¥–∏–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É ¬´—Å—ã—Ä—ã–º¬ª –≤–µ—Å–æ–º (BF16) –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º (NF4).\n",
    "*   **–ò—Å—Å–ª–µ–¥—É–µ–º –¥–≤–∏–∂–∫–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:** –ú—ã –ø—Ä–æ–π–¥–µ–º –ø—É—Ç—å –æ—Ç –≥–∏–±–∫–æ–≥–æ, –Ω–æ —Ç—è–∂–µ–ª–æ–≥–æ `Transformers`, —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π `Unsloth`, –¥–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ `Llama.cpp` –∏ –≥–æ—Ç–æ–≤–æ–≥–æ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É `Ollama`.\n",
    "*   **–ò–∑—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã:** –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∂–∏–º–µ ¬´—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è¬ª (**Thinking Mode**) –∏ —Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –∑–∞–¥–µ—Ä–∂–∫—É –∏ —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (TPS).\n",
    "*   **–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–µ–∂–∏–º—ã:** –£–≤–∏–¥–∏–º –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç –≤–ª–µ–∑–∞—Ç—å –≤ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å –∏ –∑–∞–¥–µ–π—Å—Ç–≤—É–µ—Ç —Ä–µ—Å—É—Ä—Å—ã –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242ced7",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç—ã –∏ –£—Ç–∏–ª–∏—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b520d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "üìä VRAM: –ó–∞–Ω—è—Ç–æ 0.00 –ì–ë | –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ 0.00 –ì–ë\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from tabulate import tabulate\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import openai\n",
    "\n",
    "# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤\n",
    "results = []\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ VRAM –∏ Garbage Collector\"\"\"\n",
    "    # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–µ–π, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Å—Ç–∞–ª–∏—Å—å –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏\n",
    "    for var in ['model', 'tokenizer', 'llm', 'llm_hybrid']:\n",
    "        if var in globals():\n",
    "            del globals()[var]\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "    print(\"üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"–í—ã–≤–æ–¥ –∑–∞–Ω—è—Ç–æ–π –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        print(f\"üìä VRAM: –ó–∞–Ω—è—Ç–æ {allocated:.2f} –ì–ë | –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ {reserved:.2f} –ì–ë\")\n",
    "    else:\n",
    "        print(\"‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adc481",
   "metadata": {},
   "source": [
    "## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ (–¢–µ–æ—Ä–∏—è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60e5376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üõë –ñ–µ–ª–µ–∑–æ: RTX 4070 (–õ–∏–º–∏—Ç 8 GB) ---\n",
      "ü§ñ Qwen3-8B (fp16): –í–µ—Å ~16.0 GB + –ö—ç—à ~1.2 GB = –ò–¢–û–ì–û: 17.23 GB\n",
      "ü§ñ Qwen3-8B (int4): –í–µ—Å ~4.4 GB + –ö—ç—à ~1.2 GB = –ò–¢–û–ì–û: 5.63 GB\n",
      "ü§ñ Qwen3-32B (int4): –í–µ—Å ~17.6 GB + –ö—ç—à ~1.2 GB = –ò–¢–û–ì–û: 18.83 GB\n"
     ]
    }
   ],
   "source": [
    "def estimate_vram(params_b, quant_type=\"fp16\", context=8192):\n",
    "    # –¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "    bytes_per_param = {\n",
    "        \"fp16\": 2,      # 16 bit = 2 –±–∞–π—Ç–∞ (BF16/FP16)\n",
    "        \"int8\": 1,      # 8 bit = 1 –±–∞–π—Ç\n",
    "        \"int4\": 0.55    # 4 bit = ~0.55 –±–∞–π—Ç–∞ (—Å —É—á–µ—Ç–æ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è)\n",
    "    }\n",
    "    \n",
    "    bpp = bytes_per_param.get(quant_type, 2)\n",
    "    \n",
    "    # –í–µ—Å –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "    model_size = params_b * bpp\n",
    "    \n",
    "    # KV-Cache (–ø–∞–º—è—Ç—å –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç)\n",
    "    # –§–æ—Ä–º—É–ª–∞ –¥–ª—è Qwen3 (GQA): ~0.5 –ú–ë –Ω–∞ 1–∫ —Ç–æ–∫–µ–Ω–æ–≤ * —Å–ª–æ–∏/–≥–æ–ª–æ–≤—ã (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)\n",
    "    # –î–ª—è 8B –º–æ–¥–µ–ª–∏ –ø—Ä–∏ 8k –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —ç—Ç–æ –æ–∫–æ–ª–æ 1-1.5 –ì–ë –≤ FP16\n",
    "    kv_cache_gb = (context / 1000) * 0.15 \n",
    "    \n",
    "    total = model_size + kv_cache_gb\n",
    "    return f\"ü§ñ Qwen3-{params_b}B ({quant_type}): –í–µ—Å ~{model_size:.1f} GB + –ö—ç—à ~{kv_cache_gb:.1f} GB = –ò–¢–û–ì–û: {total:.2f} GB\"\n",
    "\n",
    "print(\"--- üõë –ñ–µ–ª–µ–∑–æ: RTX 4070 (–õ–∏–º–∏—Ç 8 GB) ---\")\n",
    "\n",
    "# 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–∫–∞–∫ –Ω–∞ HuggingFace)\n",
    "print(estimate_vram(8, \"fp16\")) \n",
    "# –í—ã–≤–æ–¥: ~17 –ì–ë. –°—Ä–∞–∑—É OOM. –°—Ç—É–¥–µ–Ω—Ç—ã –≤–∏–¥—è—Ç, –ø–æ—á–µ–º—É –Ω–µ–ª—å–∑—è –ø—Ä–æ—Å—Ç–æ —Å–¥–µ–ª–∞—Ç—å .from_pretrained()\n",
    "\n",
    "# 2. –¢–æ, —á—Ç–æ –º—ã –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å (NF4 / Unsloth)\n",
    "print(estimate_vram(8, \"int4\")) \n",
    "# –í—ã–≤–æ–¥: ~5.5 –ì–ë. –ò–¥–µ–∞–ª—å–Ω–æ –≤–ª–µ–∑–∞–µ—Ç, –æ—Å—Ç–∞–µ—Ç—Å—è ~2.5 –ì–ë –ø–æ–¥ —Å–∏—Å—Ç–µ–º—É –∏ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n",
    "\n",
    "# 3. –ú–µ—á—Ç–∞ (32B –º–æ–¥–µ–ª—å)\n",
    "print(estimate_vram(32, \"int4\")) \n",
    "# –í—ã–≤–æ–¥: ~18 –ì–ë. –ù–∞ –Ω–æ—É—Ç–±—É–∫–µ –±–µ–∑ —à–∞–Ω—Å–æ–≤ (—Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ –º–µ–¥–ª–µ–Ω–Ω—ã–π RAM offload)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08fd063",
   "metadata": {},
   "source": [
    "## Qwen3-4B –≤ –ø–æ–ª–Ω–æ–º –≤–µ—Å–µ (BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e019115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n",
      "üèãÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen3-4B –≤ –ø–æ–ª–Ω–æ–º –≤–µ—Å–µ (BF16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc5e78fadcb434c92803a3512d552c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[accelerate.big_modeling|WARNING]Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä VRAM: –ó–∞–Ω—è—Ç–æ 6.37 –ì–ë | –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ 6.39 –ì–ë\n",
      "‚è± –°–∫–æ—Ä–æ—Å—Ç—å (BF16): 3.00 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\n",
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "clean_memory()\n",
    "\n",
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "print(f\"üèãÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ {model_id} –≤ –ø–æ–ª–Ω–æ–º –≤–µ—Å–µ (BF16)...\")\n",
    "\n",
    "# device_map=\"auto\" —Å–ø–∞—Å–µ—Ç –æ—Ç –∫—Ä–∞—à–∞, –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ —á—É—Ç—å –Ω–µ —Ö–≤–∞—Ç–∏—Ç\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print_memory()\n",
    "\n",
    "# –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "input_text = \"Explain the concept of entropy in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "end = time.time()\n",
    "\n",
    "# –°—á–∏—Ç–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å\n",
    "new_tokens = len(output[0]) - len(inputs.input_ids[0])\n",
    "tps = new_tokens / (end - start)\n",
    "\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å (BF16): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-4B (BF16)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500c8ac",
   "metadata": {},
   "source": [
    "## Qwen3-4B –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è (NF4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1e64b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n",
      "ü§è –ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen3-4B –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ NF4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c3d9dd3b9340e48400cdea4a7931ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä VRAM: –ó–∞–Ω—è—Ç–æ 2.65 –ì–ë | –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ 3.20 –ì–ë\n",
      "‚è± –°–∫–æ—Ä–æ—Å—Ç—å (NF4): 24.51 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\n",
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "clean_memory()\n",
    "\n",
    "print(f\"ü§è –ó–∞–≥—Ä—É–∑–∫–∞ {model_id} –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ NF4...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print_memory() # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 3 –ì–ë\n",
    "\n",
    "inputs = tokenizer(\"Explain the concept of entropy in simple terms.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "end = time.time()\n",
    "\n",
    "new_tokens = len(output[0]) - len(inputs.input_ids[0])\n",
    "tps = new_tokens / (end - start)\n",
    "\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å (NF4): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-4B (NF4)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b28e47",
   "metadata": {},
   "source": [
    "## Qwen3-8B (Transformers + Thinking Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1606b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n",
      "üß† –ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen3-8B (HF NF4) —Å Thinking Mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dfa5aa8fd94bc39c15c7df9fa4e425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAW OUTPUT ---\n",
      "<|im_start|>user\n",
      "How many 'r's are in the word 'Strawberry'?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, let's see. The user is asking how many 'r's are in the word 'Strawberry'. Hmm, first I need to make sure I spell the word correctly. Let me write it out: S-T-R-A-W-B-E-R-R-Y. Wait, let me check again. S-T-R-A-W-B-E-R-R-Y. Yes, that's right. Now, I need to count the number of times the letter 'r' appears.\n",
      "\n",
      "Starting from the beginning: S is the first letter, then T, then R. So that's one 'r'....\n",
      "\n",
      "‚è± –°–∫–æ—Ä–æ—Å—Ç—å (HF 8B): 26.64 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\n",
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "clean_memory()\n",
    "\n",
    "model_id_8b = \"Qwen/Qwen3-8B\"\n",
    "print(f\"üß† –ó–∞–≥—Ä—É–∑–∫–∞ {model_id_8b} (HF NF4) —Å Thinking Mode...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_8b,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_8b)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Thinking Mode\n",
    "prompt = \"How many 'r's are in the word 'Strawberry'?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º enable_thinking=True\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True \n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "end = time.time()\n",
    "\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"\\n--- RAW OUTPUT ---\")\n",
    "print(output_text[:500] + \"...\") \n",
    "\n",
    "new_tokens = len(generated_ids[0]) - len(model_inputs.input_ids[0])\n",
    "tps = new_tokens / (end - start)\n",
    "\n",
    "print(f\"\\n‚è± –°–∫–æ—Ä–æ—Å—Ç—å (HF 8B): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-8B (HF NF4)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cab865",
   "metadata": {},
   "source": [
    "## Qwen3-8B —á–µ—Ä–µ–∑ Unsloth (–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51fccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n",
      "üêá Unsloth: –ó–∞–≥—Ä—É–∑–∫–∞ unsloth/Qwen3-8B-bnb-4bit...\n",
      "==((====))==  Unsloth 2026.1.4: Fast Qwen3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.633 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚è± –°–∫–æ—Ä–æ—Å—Ç—å (Unsloth 8B): 30.68 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\n",
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "clean_memory()\n",
    "\n",
    "unsloth_model_id = \"unsloth/Qwen3-8B-bnb-4bit\" \n",
    "\n",
    "print(f\"üêá Unsloth: –ó–∞–≥—Ä—É–∑–∫–∞ {unsloth_model_id}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=unsloth_model_id,\n",
    "    max_seq_length=4096,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer([\"How many 'r's are in the word 'Strawberry'?\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# –ü—Ä–æ–≥—Ä–µ–≤\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "end = time.time()\n",
    "\n",
    "tps = 512 / (end - start) # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å (Unsloth 8B): {tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "results.append([\"Qwen3-8B (Unsloth)\", f\"{torch.cuda.memory_allocated(0)/1024**3:.2f} GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del model, tokenizer\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f156ef",
   "metadata": {},
   "source": [
    "## Llama.cpp (GGUF & Hybrid Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08849d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Llama-cpp-python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA\n",
    "# –í–ê–ñ–ù–û: –ë–µ–∑ CMAKE_ARGS –æ–Ω–∞ –≤—Å—Ç–∞–Ω–µ—Ç –∫–∞–∫ CPU-only –±–∏–±–ª–∏–æ—Ç–µ–∫–∞.\n",
    "# CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "245c4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n",
      "üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ GGUF...\n",
      "–§–∞–π–ª –≥–æ—Ç–æ–≤: models/Qwen3-8B-Q4_K_M.gguf\n",
      "\n",
      "--- Llama.cpp: Full GPU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ç–≤–µ—Ç (GPU): <think>\n",
      "Okay, I need to explain Rust ownership. Le...\n",
      "‚è± –°–∫–æ—Ä–æ—Å—Ç—å: 41.31 t/s\n",
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n",
      "\n",
      "--- Llama.cpp: Hybrid (CPU+GPU) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ç–≤–µ—Ç (Hybrid): <think>\n",
      "Okay, the user is asking about the Python ...\n",
      "‚è± –°–∫–æ—Ä–æ—Å—Ç—å: 5.48 t/s\n",
      "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "clean_memory()\n",
    "\n",
    "# 1. –°–∫–∞—á–∏–≤–∞–µ–º GGUF —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É ./models\n",
    "print(\"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ GGUF...\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"unsloth/Qwen3-8B-GGUF\", \n",
    "    filename=\"Qwen3-8B-Q4_K_M.gguf\",\n",
    "    local_dir=\"./models\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"–§–∞–π–ª –≥–æ—Ç–æ–≤: {model_path}\")\n",
    "\n",
    "# 2. –¢–µ—Å—Ç Full GPU\n",
    "print(\"\\n--- Llama.cpp: Full GPU ---\")\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1, \n",
    "    n_ctx=4096,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain rust ownership.\"}],\n",
    "    max_tokens=256\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "new_tokens = output['usage']['completion_tokens']  # pyright: ignore[reportIndexIssue]\n",
    "tps = new_tokens / (end - start)\n",
    "print(f\"–û—Ç–≤–µ—Ç (GPU): {output['choices'][0]['message']['content'][:50]}...\")  # pyright: ignore[reportIndexIssue, reportOptionalSubscript]\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å: {tps:.2f} t/s\")\n",
    "\n",
    "# –î–ª—è GGUF —Å–ª–æ–∂–Ω–æ —Ç–æ—á–Ω–æ –∑–∞–º–µ—Ä–∏—Ç—å VRAM –∏–∑ Python, —Å—Ç–∞–≤–∏–º –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–ª–∏ 0\n",
    "results.append([\"Qwen-8B (Llama.cpp GPU)\", \"~5.5 GB\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del llm\n",
    "clean_memory()\n",
    "\n",
    "# 3. –¢–µ—Å—Ç Hybrid (CPU Offload)\n",
    "print(\"\\n--- Llama.cpp: Hybrid (CPU+GPU) ---\")\n",
    "llm_hybrid = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=10, # –ú–∞–ª–æ —Å–ª–æ–µ–≤ –≤ GPU\n",
    "    n_ctx=4096,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "output = llm_hybrid.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain python GIL.\"}],\n",
    "    max_tokens=256\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "new_tokens = output['usage']['completion_tokens']  # pyright: ignore[reportIndexIssue]\n",
    "tps = new_tokens / (end - start)\n",
    "print(f\"–û—Ç–≤–µ—Ç (Hybrid): {output['choices'][0]['message']['content'][:50]}...\")  # pyright: ignore[reportOptionalSubscript, reportIndexIssue]\n",
    "print(f\"‚è± –°–∫–æ—Ä–æ—Å—Ç—å: {tps:.2f} t/s\")\n",
    "results.append([\"Qwen-8B (Hybrid CPU+GPU)\", \"~2.0 GB VRAM\", f\"{tps:.2f} t/s\"])\n",
    "\n",
    "del llm_hybrid\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d092a6",
   "metadata": {},
   "source": [
    "## Ollama + OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47931e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Ollama (Docker): –ó–∞–ø—É—Å–∫ —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º...\n",
      "–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n",
      "**Ode to Linux**  \n",
      "\n",
      "Core of code, a community's embrace,  \n",
      "Freedom's call, security's grace.  \n",
      "A kernel's heart, where commands align,  \n",
      "In circuits deep, it hums with might.  \n",
      "\n",
      "From servers tall to desktop's light,  \n",
      "It bends and grows, a versatile sprite.  \n",
      "In terminals deep, the power's near,  \n",
      "A world of tools, both old and new.  \n",
      "\n",
      "Open-source roots, a shared design,  \n",
      "Where every line is a collective sign.  \n",
      "Contributors' hands, both near and far,  \n",
      "Forge a future that's evermore star.  \n",
      "\n",
      "So here's to Linux, steadfast and true,  \n",
      "A digital realm where freedom's hue.  \n",
      "In every byte, it stands anew.\n",
      "\n",
      "‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: ~6.15 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "print(\"üåê Ollama (Docker): –ó–∞–ø—É—Å–∫ —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º...\")\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\" \n",
    ")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. –í–∫–ª—é—á–∞–µ–º stream=True\n",
    "    # 2. –£–±–∏—Ä–∞–µ–º max_tokens (–ø—É—Å—Ç—å –≥–æ–≤–æ—Ä–∏—Ç —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"qwen3:8b\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a short poem about Linux.\"}\n",
    "        ],\n",
    "        stream=True,  \n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\")\n",
    "    collected_messages = []\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º –ø–æ—Ç–æ–∫ –ø–æ –∫—É—Å–æ—á–∫–∞–º\n",
    "    for chunk in stream:\n",
    "        chunk_message = chunk.choices[0].delta.content\n",
    "        if chunk_message:\n",
    "            print(chunk_message, end=\"\", flush=True) # –ü–µ—á–∞—Ç–∞–µ–º —Å—Ä–∞–∑—É\n",
    "            collected_messages.append(chunk_message)\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º\n",
    "    full_text = \"\".join(collected_messages)\n",
    "    token_estimate = len(full_text.split()) * 1.3\n",
    "    duration = end_time - start_time\n",
    "    tps = token_estimate / duration\n",
    "    \n",
    "    print(f\"\\n\\n‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: ~{tps:.2f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É\n",
    "    if 'results' in globals():\n",
    "        results.append([\"Qwen-8B (Ollama)\", \"Docker / Streaming\", f\"~{tps:.2f} t/s\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå –û—à–∏–±–∫–∞: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f2787",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780cae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ (RTX 4070 Laptop)\n",
      "| –ú–æ–¥–µ–ª—å / –ú–µ—Ç–æ–¥           | VRAM               | –°–∫–æ—Ä–æ—Å—Ç—å   |\n",
      "|--------------------------|--------------------|------------|\n",
      "| Qwen3-4B (BF16)          | 6.38 GB            | 3.00 t/s   |\n",
      "| Qwen3-4B (NF4)           | 2.65 GB            | 24.51 t/s  |\n",
      "| Qwen3-8B (HF NF4)        | 6.02 GB            | 26.64 t/s  |\n",
      "| Qwen3-8B (Unsloth)       | 5.82 GB            | 30.68 t/s  |\n",
      "| Qwen-8B (Llama.cpp GPU)  | ~5.5 GB            | 41.31 t/s  |\n",
      "| Qwen-8B (Hybrid CPU+GPU) | ~2.0 GB VRAM       | 5.48 t/s   |\n",
      "| Qwen-8B (Ollama)         | Docker / Streaming | ~6.15 t/s  |\n"
     ]
    }
   ],
   "source": [
    "print(\"### üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ (RTX 4070 Laptop)\")\n",
    "print(tabulate(results, headers=[\"–ú–æ–¥–µ–ª—å / –ú–µ—Ç–æ–¥\", \"VRAM\", \"–°–∫–æ—Ä–æ—Å—Ç—å\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbc73f",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã –ø–æ –∏—Ç–æ–≥–∞–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "–ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM –≤ 2026 –≥–æ–¥—É:\n",
    "\n",
    "1.  **–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ ‚Äî —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç.** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 4-–±–∏—Ç–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (NF4, GGUF) –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (8B+) –Ω–∞ –∏–≥—Ä–æ–≤—ã—Ö –Ω–æ—É—Ç–±—É–∫–∞—Ö, –≤—ã—Å–≤–æ–±–æ–∂–¥–∞—è –¥–æ 60‚Äì70% –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ –∫–∞—á–µ—Å—Ç–≤–∞. –ë–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∑–∞–ø—É—Å–∫ SOTA-–º–æ–¥–µ–ª–µ–π –æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–∏–≤–∏–ª–µ–≥–∏–µ–π —Å–µ—Ä–≤–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.\n",
    "2.  **–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** \n",
    "    *   **Unsloth** –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Å–≤–æ—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è R&D, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–∏—Ä–æ—Å—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π –∑–∞ —Å—á–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–¥–µ—Ä Triton.\n",
    "    *   **Llama.cpp** –æ—Å—Ç–∞–µ—Ç—Å—è —ç—Ç–∞–ª–æ–Ω–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—Ç–∏–ª–∏–∑–∏—Ä—É—è —Ä–µ—Å—É—Ä—Å—ã GPU.\n",
    "3.  **–õ–æ–≤—É—à–∫–∞ \"System Memory Fallback\":** –ö–∞–∫ —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–º –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—É—é VRAM, –¥—Ä–∞–π–≤–µ—Ä –∑–∞–¥–µ–π—Å—Ç–≤—É–µ—Ç –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å. –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—à –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –Ω–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫—Ä–∞—Ç–Ω–æ–º—É –ø–∞–¥–µ–Ω–∏—é —Å–∫–æ—Ä–æ—Å—Ç–∏ (–≤ 5‚Äì10 —Ä–∞–∑), –ø—Ä–µ–≤—Ä–∞—â–∞—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç –≤ —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É.\n",
    "4.  **–°–ø–µ—Ü–∏—Ñ–∏–∫–∞ \"Thinking\" –º–æ–¥–µ–ª–µ–π:** –î–ª—è —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen3 —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ TPS —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –≤—Ä–µ–º—è ¬´—Ä–∞–∑–¥—É–º–∏–π¬ª (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π), –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –≤—ã–¥–∞—á–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.\n",
    "5.  **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø–∏—Ç–∞–Ω–∏—è (Hardware P-states):** –ù–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä—è–º—É—é –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è. –ü–µ—Ä–µ—Ö–æ–¥ GPU –≤ –Ω–∏–∑–∫–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —á–∞—Å—Ç–æ—Ç—ã (P-states) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –æ—Ç –±–∞—Ç–∞—Ä–µ–∏ –º–æ–∂–µ—Ç –Ω–∏–≤–µ–ª–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.\n",
    "6.  **–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ API:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ (Ollama / Llama.cpp) –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–µ—Å—à–æ–≤–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –æ–±–ª–∞—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–≤–∏–∂–∫–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø–æ–ª–Ω—É—é –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å.\n",
    "\n",
    "**–ò—Ç–æ–≥:** –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π —É—Ä–æ–≤–Ω—è 8B —Å–µ–≥–æ–¥–Ω—è ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –æ–±–ª–∞–∫–∞–º, –∞ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤–æ–µ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
